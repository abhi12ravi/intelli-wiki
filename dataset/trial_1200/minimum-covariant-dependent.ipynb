{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhi\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 29/29 [00:12<00:00,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Accuracy  Balanced Accuracy ROC AUC  F1 Score  \\\n",
      "Model                                                                          \n",
      "RandomForestClassifier             0.81               0.80    None      0.80   \n",
      "LGBMClassifier                     0.80               0.80    None      0.80   \n",
      "XGBClassifier                      0.80               0.80    None      0.80   \n",
      "ExtraTreesClassifier               0.79               0.78    None      0.78   \n",
      "BaggingClassifier                  0.75               0.75    None      0.75   \n",
      "DecisionTreeClassifier             0.71               0.71    None      0.71   \n",
      "KNeighborsClassifier               0.70               0.70    None      0.70   \n",
      "NuSVC                              0.69               0.69    None      0.69   \n",
      "LogisticRegression                 0.67               0.67    None      0.66   \n",
      "AdaBoostClassifier                 0.67               0.67    None      0.66   \n",
      "SVC                                0.64               0.64    None      0.64   \n",
      "ExtraTreeClassifier                0.65               0.64    None      0.65   \n",
      "CalibratedClassifierCV             0.64               0.63    None      0.63   \n",
      "SGDClassifier                      0.61               0.61    None      0.59   \n",
      "LinearSVC                          0.61               0.61    None      0.61   \n",
      "LabelPropagation                   0.61               0.61    None      0.60   \n",
      "LabelSpreading                     0.61               0.61    None      0.60   \n",
      "LinearDiscriminantAnalysis         0.59               0.59    None      0.59   \n",
      "RidgeClassifierCV                  0.59               0.59    None      0.59   \n",
      "RidgeClassifier                    0.59               0.59    None      0.59   \n",
      "BernoulliNB                        0.55               0.54    None      0.54   \n",
      "PassiveAggressiveClassifier        0.54               0.54    None      0.51   \n",
      "NearestCentroid                    0.52               0.51    None      0.51   \n",
      "Perceptron                         0.45               0.45    None      0.41   \n",
      "QuadraticDiscriminantAnalysis      0.44               0.44    None      0.39   \n",
      "GaussianNB                         0.42               0.42    None      0.37   \n",
      "DummyClassifier                    0.26               0.26    None      0.26   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "RandomForestClassifier               0.89  \n",
      "LGBMClassifier                       0.52  \n",
      "XGBClassifier                        1.60  \n",
      "ExtraTreesClassifier                 0.65  \n",
      "BaggingClassifier                    0.26  \n",
      "DecisionTreeClassifier               0.05  \n",
      "KNeighborsClassifier                 0.14  \n",
      "NuSVC                                0.93  \n",
      "LogisticRegression                   0.14  \n",
      "AdaBoostClassifier                   0.28  \n",
      "SVC                                  0.71  \n",
      "ExtraTreeClassifier                  0.01  \n",
      "CalibratedClassifierCV               3.28  \n",
      "SGDClassifier                        0.10  \n",
      "LinearSVC                            0.81  \n",
      "LabelPropagation                     0.54  \n",
      "LabelSpreading                       0.85  \n",
      "LinearDiscriminantAnalysis           0.03  \n",
      "RidgeClassifierCV                    0.03  \n",
      "RidgeClassifier                      0.02  \n",
      "BernoulliNB                          0.02  \n",
      "PassiveAggressiveClassifier          0.04  \n",
      "NearestCentroid                      0.02  \n",
      "Perceptron                           0.03  \n",
      "QuadraticDiscriminantAnalysis        0.02  \n",
      "GaussianNB                           0.03  \n",
      "DummyClassifier                      0.01  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import lazypredict\n",
    "import sys\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "#Read data file\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "filepath = \"balanced_dataset_MASTER.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "features = df\n",
    "\n",
    "# Labels are the values we want to predict\n",
    "labels = np.array(df['protection_level'])\n",
    "\n",
    "# 0 => unprotected\n",
    "# 1 => autoconfirmed\n",
    "# 2 => extendedconfirmed\n",
    "# 3 => sysop\n",
    "labels_encoded = []\n",
    "for item in labels:\n",
    "    if(item ==\"unprotected\"):\n",
    "        labels_encoded.append(0)\n",
    "    elif(item == \"autoconfirmed\"):\n",
    "        labels_encoded.append(1)\n",
    "    elif(item == \"extendedconfirmed\"):\n",
    "        labels_encoded.append(2)\n",
    "    elif(item == \"sysop\"):\n",
    "        labels_encoded.append(3)  \n",
    "labels_encoded\n",
    "\n",
    "# Remove the labels from the features\n",
    "features = features.drop('protection_level', axis = 1)\n",
    "features = features.drop('page_title', axis = 1)\n",
    "features = features.drop('protection_expiry', axis = 1)\n",
    "# features = features.drop('page_id', axis = 1)\n",
    "\n",
    "# Replace NaN\n",
    "features = features.replace('Fewer than 30 watchers',np.NaN)\n",
    "features = features.replace('There may or may not be a watching user visiting recent edits',np.NaN)\n",
    "\n",
    "#Convert cols to Float\n",
    "features['page_length'] = features['page_length'].astype(float)\n",
    "features['total_edits'] = features['total_edits'].astype(float)\n",
    "features['number_page_watchers'] = features['number_page_watchers'].astype(float)\n",
    "features['number_page_watchers_recent_edits'] = features['number_page_watchers_recent_edits'].astype(float)\n",
    "\n",
    "# Saving feature names for later use\n",
    "feature_list = list(features.columns)\n",
    "\n",
    "# Convert to numpy array\n",
    "features = np.array(features)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels_encoded, test_size =0.20, random_state = 53)\n",
    "\n",
    "X_train = train_features\n",
    "y_train = train_labels\n",
    "X_test = test_features\n",
    "y_test = test_labels\n",
    "\n",
    "\n",
    "\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\n",
    "models,predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier removal - minimum covariant dependant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "features = imputer.fit_transform(features)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels_encoded, test_size =0.20, random_state = 53)\n",
    "\n",
    "X_train = train_features\n",
    "y_train = train_labels\n",
    "X_test = test_features\n",
    "y_test = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3813, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import EllipticEnvelope\n",
    "# identify outliers in the training dataset\n",
    "ee = EllipticEnvelope(contamination=0.01)\n",
    "yhat = ee.fit_predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = yhat != -1\n",
    "y_train = np.array(y_train)\n",
    "X_train, y_train = X_train[mask, :], y_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3774, 11)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.5115%\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier implementation \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rf = RandomForestClassifier(max_depth=13, random_state=0, criterion='gini', oob_score= True, n_jobs=4)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "#Predict\n",
    "y_pred = rf.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "\n",
    "\n",
    "#Evaluate\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.4f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
